{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 NeuralNW pytorch udacity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "rVWLzfGiKleQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnJbeQWEFTjF",
        "colab_type": "text"
      },
      "source": [
        "###imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plkA3n87FYge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBlyf5Fc6E3a",
        "colab_type": "text"
      },
      "source": [
        "### sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKpL0U4v58WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return(1/1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uhoi05KFpso",
        "colab_type": "text"
      },
      "source": [
        "### create random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWJlHakSFfqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "features = torch.randn((1,5))\n",
        "wts = torch.randn_like(features)\n",
        "bias = torch.randn((1,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0FFPlMjGBRE",
        "colab_type": "code",
        "outputId": "b41d222e-cdcb-4ec4-fe05-e86e156ba6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = sigmoid(torch.sum(features*wts)+bias)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6616]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4f81-_GJJQ",
        "colab_type": "code",
        "outputId": "7f0b7c89-a198-4a4c-8bfa-2ee896cbe6d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = sigmoid(torch.mm(features,wts.view(5,1))+bias)  # view will convert to desired output\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6616]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfaBZpU4Ga3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating 3 inputs 1 hidden layer and 1 output unit\n",
        "\n",
        "# Features are 3 random normal variables\n",
        "features = torch.randn((1, 3))\n",
        "\n",
        "# Define the size of each layer in our network\n",
        "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
        "n_hidden = 2                    # Number of hidden units \n",
        "n_output = 1                    # Number of output units\n",
        "\n",
        "# Weights for inputs to hidden layer\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "# Weights for hidden layer to output layer\n",
        "W2 = torch.randn(n_hidden, n_output)\n",
        "\n",
        "# and bias terms for hidden and output layers\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8jtdxt6H705",
        "colab_type": "code",
        "outputId": "09b7efa3-5d9c-4749-cd73-7cbce84fe788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "h = sigmoid(torch.mm(features,W1)+B1)\n",
        "output = sigmoid(torch.mm(h,W2)+B2)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0742]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK8HrrSKxDZ",
        "colab_type": "text"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b31UIqFkKwmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsUaf8fGIsH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets,transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,))  # mean and std deviation\n",
        "                               ])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/',download = True,train = True,transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 64,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AP5XxCDG0t2",
        "colab_type": "code",
        "outputId": "c2ba4e51-ae6a-4262-a4d4-20e34eb5847a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WecaygJ0PVTF",
        "colab_type": "code",
        "outputId": "b6d9741c-702c-422a-f981-1ac3cde00c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# since we need probabilities of each class we use softmax with 10 outputs\n",
        "\n",
        "def softmax(x):\n",
        "  return torch.exp(x)/torch.sum(torch.exp(x),dim =1).view(-1,1)  # dim = 1 sum across columns\n",
        "\n",
        "\n",
        "'''\n",
        "if torch.exp(x) will have 64 X 10 dimension ; torch.sum(torch.exp(x),dim =1) will give tensor of 64 elements dimension will be(64,) we need (64,1)\n",
        "normally torch.exp(x)/torch.sum(torch.exp(x),dim =1) will try to divide torch.exp with all 64 elements of torch.sum()\n",
        "so we do view(-1,1) it will make 64 rows for sum with each row having 1 value row so each value in torch.exp() matches with sum\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorch.exp(x) will have 64 X 10 dimension ; torch.sum(torch.exp(x),dim =1) will give tensor of 64 elements\\nnormally torch.exp(x)/torch.sum(torch.exp(x),dim =1) will try to divide torch.exp with all 64 elements of torch.sum()\\nso we do view(-1,1) it will make 64 rows for sum with each row having 1 value row so each value in torch.exp() matches with sum\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4QRar8ND_Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#flatten input images\n",
        "inputs = images.view(images.shape[0],-1) # can also replace -1 with (784) x.shape[0] gives us batch size\n",
        "\n",
        "# create inputs 784\n",
        "w1 = torch.randn(784,256)  # 256 hidden layers\n",
        "b1 = torch.randn(256)\n",
        "\n",
        "w2 = torch.randn(256,10) # 10 output units\n",
        "b2 = torch.randn(10)\n",
        "\n",
        "h = softmax(torch.mm(inputs,w1) + b1)\n",
        "out = torch.mm(h,w2)+b2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNAJe-E6M23U",
        "colab_type": "text"
      },
      "source": [
        "## inbuilt method for nn classifying  fashion mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy42QS-DHj64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import helper\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1TYMX5dP2ox",
        "colab_type": "code",
        "outputId": "082a120d-627d-42c0-cf33-4f7880289b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "# need helper script link down below\n",
        "image, label = next(iter(trainloader))\n",
        "helper.imshow(image[0,:]);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-f4afd08dbb7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'imshow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtUoiabQP4gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn,optim\n",
        "import torch.nn.functional as F # functional will make our work easy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ekRzvfIGyn",
        "colab_type": "text"
      },
      "source": [
        "## classifier defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVWi4tu6QJUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784,256) # creating a single hidden layer\n",
        "    self.fc2 = nn.Linear(256,128)\n",
        "    self.fc3 = nn.Linear(128,64)\n",
        "    self.fc4 = nn.Linear(64,10)\n",
        "  \n",
        "  def forward(self,x): #forward pass\n",
        "    #print(x.shape) torch.Size([64, 1, 28, 28])\n",
        "    x = x.view(x.shape[0],-1)     # flatten input \n",
        "    #print(x.shape)  torch.Size([64, 784])\n",
        "    x = F.relu(self.fc1(x))       # apply activation function to layer\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.log_softmax(self.fc4(x),dim =1)  # apply log softmax for output layer where \n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOYyKPXFSQ3W",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qq_PDvxRYoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.003)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc2mMRZbSmyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images,labels in trainloader:\n",
        "    log_ps = model(images)\n",
        "    loss = criterion(log_ps,labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()    # backward pass \n",
        "    optimizer.step()  # to update the weights\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  \n",
        "  else:  \n",
        "    print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DbEyeayTslN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "img = images[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjel1oyiUFRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0TOMlZSUMzo",
        "colab_type": "code",
        "outputId": "9b035aa1-4056-428d-8bfe-66b0d443119c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.imshow(img.reshape(28,28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed1ebd61d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfiJPsfxEnmL5iTz1fxACJlVFBHw+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8LOBLz4UOUe",
        "colab_type": "code",
        "outputId": "3b8564c4-79b6-4298-d682-132e41719524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "output = torch.exp(model(img))\n",
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8480e-01, 1.0488e-05, 1.3338e-03, 1.8327e-03, 1.8539e-05, 7.7281e-08,\n",
              "         1.1197e-01, 1.1084e-10, 3.1550e-05, 1.9243e-10]],\n",
              "       grad_fn=<ExpBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BkdMIFLav8E",
        "colab_type": "code",
        "outputId": "075cda88-43bb-4a68-dfd4-542d280ca9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "output.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8480e-01, 1.0488e-05, 1.3338e-03, 1.8327e-03, 1.8539e-05, 7.7281e-08,\n",
              "         1.1197e-01, 1.1084e-10, 3.1550e-05, 1.9243e-10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh93LLQ6bGL_",
        "colab_type": "code",
        "outputId": "12bd5324-77c5-45e7-fb4a-2a47eab03b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "output.data.numpy().squeeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.8479960e-01, 1.0488332e-05, 1.3337604e-03, 1.8326698e-03,\n",
              "       1.8538793e-05, 7.7280809e-08, 1.1197327e-01, 1.1083554e-10,\n",
              "       3.1550229e-05, 1.9242787e-10], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9uP7y-ZuzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9IZQ8qlaDNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code from above\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgiF2uk0UaM9",
        "colab_type": "code",
        "outputId": "ce97f23f-dd1d-463a-f9b4-55da00c1c985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "view_classify(img, output, version='Fashion')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADPCAYAAACgNEWWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noMzMrJBcoMzMrJD+HwRQKpvXFY40AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R38OLjmUUdU4",
        "colab_type": "code",
        "outputId": "ab6cfb1d-7d6f-4a0f-db30-e8d0d72f5904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"actual \",label[1])\n",
        "print(\"predicted\",preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actual  tensor(9)\n",
            "predicted 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7bSKMOHwB1",
        "colab_type": "text"
      },
      "source": [
        "### inference and validation\n",
        "\n",
        "inference  = prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogxbQDGrUlDy",
        "colab_type": "code",
        "outputId": "d9bd80a0-24f6-4868-8d55-732e41b6da06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Classifier()\n",
        "\n",
        "#loading test set\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities\n",
        "ps = torch.exp(model(images))   # output of the model trained\n",
        "\n",
        "\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0EvPi9GIs_K",
        "colab_type": "code",
        "outputId": "45f766c8-10bb-45e5-e364-8ec38971f2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#printing top class in the dataset \n",
        "\n",
        "top_p, top_class = ps.topk(1,dim =1) #will print top k classes returns top k numbers and corrs classes\n",
        "\n",
        "print(\"top 10 values\",top_class[:10,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 10 values tensor([[2],\n",
            "        [2],\n",
            "        [9],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [9],\n",
            "        [2],\n",
            "        [2]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl1lppReJc8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#comparing output with our real values ie. label with top_class\n",
        "# but label is 1d tensor of len 64\n",
        "# top_class is 2d tensor of dim 64,1\n",
        "\n",
        "\n",
        "equals = top_class == label.view(-1,1)\n",
        "\n",
        "equals = top_class == label.view(*top_class.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqv_UgiaKkpe",
        "colab_type": "text"
      },
      "source": [
        "### finding accuracy :\n",
        "simply do sum of all values / no of all values === mean\n",
        "\n",
        "here equals = bool but mean works with int+ so convert equals to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knrW-WigKM5l",
        "colab_type": "code",
        "outputId": "19e59056-f931-4b05-fc3a-cfdada679c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "print(f'Accuracy score is : {accuracy.item()*100}%')\n",
        "\n",
        "#without training our model just random accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is : 14.0625%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gM0Oc1XU9hj",
        "colab_type": "text"
      },
      "source": [
        "# For training set and validation set and adding drop out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_vzsZDpYDRN",
        "colab_type": "text"
      },
      "source": [
        "##imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp9HGOQBYG7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUEpya3iHgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHOCTKLEYQWH",
        "colab_type": "text"
      },
      "source": [
        "## classifier class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqN6l4B5U9Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# avoiding overfitting using dropout\n",
        "\n",
        "class Classifier2(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 10)\n",
        "    \n",
        "    self.dropout = nn.Dropout(p = 0.2)   # added drop out\n",
        "    \n",
        "    \n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    # print(x.shape)\n",
        "    x = x.view(x.shape[0],-1) #flattening x\n",
        "    # print(x.shape)\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    \n",
        "    # last output will be using softmax for prob outputs\n",
        "    x = F.log_softmax(self.fc4(x),dim =1)\n",
        "    \n",
        "    output = x\n",
        "    print(\"********************************\",type(output),output,type(x),x)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW9PUhDWZhIs",
        "colab_type": "text"
      },
      "source": [
        "during training only we use drop out during inference we use whole network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLx3GrwaKPp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier2()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.003)\n",
        "\n",
        "epochs = 30\n",
        "steps = 0\n",
        "\n",
        "train_losses,test_losses = [],[]\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images,labels in trainloader:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    log_ps = model(images)\n",
        "    loss = criterion(log_ps,labels)\n",
        "    loss.backward() #this is for backward propogation \n",
        "    optimizer.step()  # this is for the steps that will be taken backwards\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "    \n",
        "    \n",
        "  else :\n",
        "    test_loss = 0\n",
        "    accuracy =0\n",
        "    \n",
        "    # turn off gradients for validation, saving memory and computation\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      model.eval() #setting model to evaluation mode\n",
        "      for images,labels in testloader:\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps,labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p,top_class = ps.topk(1,dim =1)  # finding top n classes in ouput gives top class labels and values\n",
        "        print(top_p, top_class)\n",
        "        \n",
        "        #shape of top_class = 64,1 and label is 1D tensor with shape 64, so we make labels into shape of top_class\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        #equals2 = top_class == labels.view(1,-1)\n",
        "        \n",
        "        print(f\"top_class shape : {top_class.shape} without * {top_class.shape}\")\n",
        "        print(f\"equals value orig : {equals}\")\n",
        "        \n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))  # equals is in boolean convert to float\n",
        "    \n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    train_losses.append(running_loss/len(trainloader))\n",
        "    test_losses.append(test_loss/len(testloader))\n",
        "    \n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS_ANf2NgXu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlPk65AdonC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxHVrQbC4EFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpeQqVgEFU5k",
        "colab_type": "text"
      },
      "source": [
        "# saving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo6puSggFUkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('name',path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twyCt7YGHV3",
        "colab_type": "text"
      },
      "source": [
        "# Loading images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gauq5tj7GJms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#import helper\n",
        "\n",
        "#keep images in format data/train data/test\n",
        "\n",
        "data_dir = 'hello/'\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                          transforms.RandomRotation(30),\n",
        "                                        transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor() ])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "\n",
        "# DataLoaders take input you provide and returns them back in batches with corresponding labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nSnmfVFHeSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading train and testdata one by one as they need iterator \n",
        "\n",
        "data_iter = iter(testloader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    helper.imshow(images[ii], ax=ax, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohi5ZrYcJfpq",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADQgyi8XJkC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "'''\n",
        "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization \n",
        "used when the models were trained. Each color channel was normalized separately, \n",
        "the means are [0.485, 0.456, 0.406] and the standard deviations are [0.229, 0.224, 0.225].\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVWLzfGiKleQ",
        "colab_type": "text"
      },
      "source": [
        "### loading images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpYF5WE8Jz_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'Cat_Dog_data'\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPkG5Ds4J6PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "037YpElTJ9lm",
        "colab_type": "text"
      },
      "source": [
        "## new method of making a model than above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQo2qOkEKB87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "da3185cb-3670-4a02-8cf9-ad089df3f313"
      },
      "source": [
        "## Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "from collections import OrderedDict\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                            ('fc1',nn.Linear(1024,500)),\n",
        "                            ('relu',nn.ReLU()),\n",
        "                            ('fc2', nn.Linear(500, 2)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "    \n",
        "model.classifier = classifier \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3cc1feba7d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuvN4qreLPmD",
        "colab_type": "text"
      },
      "source": [
        "## or can use this method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QoCuo9hKMht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "model.classifier = nn.Sequential(nn.Linear(1024, 256),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(0.2),\n",
        "                                 nn.Linear(256, 2),\n",
        "                                 nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
        "\n",
        "model.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhuzjb8fLhvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model.forward(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in testloader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    logps = model.forward(inputs)\n",
        "                    batch_loss = criterion(logps, labels)\n",
        "                    \n",
        "                    test_loss += batch_loss.item()\n",
        "                    \n",
        "                    # Calculate accuracy\n",
        "                    ps = torch.exp(logps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                    \n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}